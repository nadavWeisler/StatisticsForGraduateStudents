---
title: "Final Project"
author: "Nadav Weisler - 316493758, Gaya Aran - 209636885"
output: html_document
---

```{r warning=FALSE}
require(rjson)
require(corrplot)
require(ez)
require(lsr)
require(DescTools)
require(sciplot)
require(pwr)
require(WebPower)
require(effectsize)
require(sjPlot)
require(lme4)
require(lmerTest)
require(ggplot2)
require(simr)
require(ggeffects)
```

```{r}
# setwd("SOME_PATH")
```

# Part 1: Describing the design and dataset [7 points overall]

## 1.1 [300 words overall; 7 points]

### 1.1.1 

The study's main research question was: how does different kinds of processing
("What" stream and "Where" stream), and the frequency of the stimulus 
presentation (10Hz and 6Hz), influence the breaking suppression time (BT/RT).

### 1.1.2

The study was experimental and had a within-subject design.

### 1.1.3 

The paradigm that was used in this study was bRMS (breaking Repeated Mask
Suppression; Abir & Hassin, 2020). The conditions in the task had a 2 (What/
Where) X 2 (10Hz/6Hz) design – which also represents the design of the entire
study.

In the standard "Where" paradigm of bRMS, participants are asked to indicate 
where the target stimulus is (left or right, in our case), while the masking is
changed mondrians. This study added a What condition, in which the participants
indicated if the target stimulus was a straight face or an inverted face, or in
other words – "What" is the target. Additionally, the masking itself was 
changed. Instead of using one mask between each presentation of the target
stimulus, two masks were used, changing the frequency of the stimulus 
presentation from the standard 10Hz to 6Hz.

### 1.1.4 

The study's dependent variable was breaking suppression time (BT), the 
independent variables were processing types ("what" and "where") and the 
frequency of the stimulus presentation (10Hz and 6Hz). A control variable (was
measured and controled it the design but not in the analysis) was face position
(inverted or straight).

### 1.1.5

The participants were recruited through Prolific, and were 100 American adults,
with no history of severe head trauma, native English speakers. Because of the
exploratory nature of the study, no power analysis was conducted and the sample
size was due to mainly convenience and the desire for a large enough sample.

### 1.1.6 

Another methodological detail you should be aware of is the face inversion 
effect – a well established effect that shows within subject BT's of straight 
faces trials will be shorter than BT's of inverted faces trials.

# Part 2: What was done already? [45 points overall]

## 2.1 [80 words; 4 points]

At the participants level – 5 participants removed due to not completing the 
experiment, and another 28 subjects whose overall accuracy percentage (not by 
condition but all the blocks together) was below 75% were filtered out.

At the trials level – incorrect trials were excluded from analysis, trials with 
BTs less than 200ms were excluded, and trials in which BT/RT was more than 3 sd 
from the participant's mean were excluded – while dividing into each block and 
calculating sd for each condition separately, including control trials (which 
are actually the training).

LOG transformation was done on the RT's, but not used for analysis.

## 2.2 [4 points]


```{r}
# Load data file
prev_df = read.csv("data.csv")
```

```{r}
# Fix 5Where and 5What to 6Where and 6What
prev_df$stimulus_block[prev_df$stimulus_block == "5Where"] = "6Where"
prev_df$stimulus_block[prev_df$stimulus_block == "5What"] = "6What"

# Create blocks name vector for future use
blockNames = c('10controlWhere', '10Where', '6Where','6What', '10What')
```

```{r}
# Function that fix subjects and correctness
getSubjectAndCorrectness = function(df) {
  # Current subject indicator
  currentSubject = 0
  # Create isAnsCorrect column
  df$isAnsCorrect = 0
  # For each row in data frame
  for(i in 1:nrow(df)) {
    # Fix subjects column
    # New subject started
    if(df[i, "trial_index"] == 0) {
      # Append 1 to currentSubject
      currentSubject = currentSubject + 1
    }
    # Set subject column to relevant subject
    df[i, "subject"] = currentSubject
    
    # Create isAnsCorrect column - If subject where correct in RMS task
    # If this trial is one of the RMS blocks
    if(is.element(df[i, "stimulus_block"], blockNames)) {
      # If its where condition
      if(endsWith(df[i, "stimulus_block"], "Where")) {
        # If it is correct (left side)
        if ((df[i, "key_press"] == "Q") & 
            (df[i, "stimulus_side"] == 1)) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is correct (right side) 
        else if ((df[i, "key_press"] == "P") & 
                 (df[i, "stimulus_side"] == 0)) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is missed (no response)
        else if (df[i, "key_press"] == -1){
          # Set -1 to isAnsCorrect
          df[i, "isAnsCorrect"] = -1
        } 
        # Else (wrong answer remains)
        else {
          # Set 0 to isAnsCorrect
          df[i, "isAnsCorrect"] = 0
        }
      } 
      # If its where condition
      else if(endsWith(df[i, "stimulus_block"], "What")) {
        # If it is correct (straight face)
        if ((df[i, "key_press"] == "Y") & 
            (startsWith(df[i, "stimulus"], "f"))) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is correct (inverted face)
        else if ((df[i, "key_press"] == "B") & 
                 (startsWith(df[i, "stimulus"], "d"))) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is missed (no response)
        else if (df[i, "key_press"] == -1){
          # Set -1 to isAnsCorrect
          df[i, "isAnsCorrect"] = -1
        } 
        # Else (wrong answer remains)
        else {
          # Set 0 to isAnsCorrect
          df[i, "isAnsCorrect"] = 0
        }
      }
    }
  }
  # Return fixed df with correctness
  return(df)
}
```

```{r}
# Update prev_df with valid subject and correctness column
prev_df = getSubjectAndCorrectness(prev_df)
```

```{r}
# Create getAccResult function, get data frame and indication if add
# control to the calculation, return Accuracy calculation that:
# Correct answers / all Answers
getAccResult = function(single_df, remove_control = F) {
  # If remove_control is true
  if(remove_control) {
    # Remove control trials from single_df
    single_df = single_df[single_df$stimulus_block != "10controlWhere",]
  } 
  # Get all incorrect and missed trials row count
  incorrect = nrow(single_df[(single_df$isAnsCorrect == 0) |
                             (single_df$isAnsCorrect == -1),])
  # Get all correct trials row count
  correct = nrow(single_df[single_df$isAnsCorrect == 1,])
  # Return accuracy result
  return((correct / (correct + incorrect)))
}
```

```{r}
# Get Missed results percent count get single_df and return missed
# results represent as missed trials count / all trial count
getMissedResult = function(single_df) {
    # Get all trials count
    all_rms_count = nrow(single_df[(single_df$isAnsCorrect == 0) | 
                                (single_df$isAnsCorrect == 1) |
                                (single_df$isAnsCorrect == -1), ])
    # Get missed trials count
    rms_misses = nrow(single_df[single_df$isAnsCorrect == -1, ])
    # return missed results
    return(rms_misses / all_rms_count)
}
```

```{r}
# Get Face inversion effect (mean of straight face minus mean of revert
# face) function, get data frame and Boolean log Return face inversion 
# effect, log values of log is true 
getFaceInversionMean = function(single_df, log=F) {
  # Get mean of rt when face is straight
  mainFaces = mean(single_df[startsWith(single_df$stimulus, "f"), ]$rt)
  # Get mean of rt when face is revert
  revertFaces = mean(single_df[startsWith(single_df$stimulus, "d"), ]$rt)
  # If log is true
  if(log) {
    # Return face inversion of log values, log rt of straight faces
    # and log rt of revert faces
    return(c(log10(mainFaces) - log10(revertFaces), 
             log(mainFaces), log10(revertFaces)))
  } else {
    # Return face inversion, rt of straight faces and rt of revert faces
    return(c(mainFaces - revertFaces, mainFaces, revertFaces))
  }
}
```

```{r}
# Get clean block data frame
getCleanInTrialDf = function(block_df) {
  # Only correct
  current_df = block_df[block_df$isAnsCorrect == 1, ]
  # Bigger than the threshold 200 ms
  current_df = current_df[current_df$rt > 200,]
  # Create scaled RT
  current_df$scaledRT = scale(current_df$rt)
  # Remove trials with rt bigger than mean +- 3 SD
  current_df = current_df[abs(current_df$scaledRT) < 3,]
  # Return clean data frame
  return(current_df)
}
```

```{r}
# Get all values for single block, get data frame and block name
# and return vector of all results for this block in this data frame
getAllValuesPerBlock = function(single_df, block_name, subject_id) {
  # Only in the relevant block
  block_df = single_df[single_df$stimulus_block == block_name,] 
  # Get clean block data frame
  current_df = getCleanInTrialDf(block_df)
  # Init results vector
  result = c()
  # Add rt mean
  result = c(result, mean(current_df$rt))
  # Add rt SD
  result = c(result, sd(current_df$rt))
  # Add accuracy
  result = c(result, getAccResult(block_df))
  # Add misses
  result = c(result, getMissedResult(block_df))
  # Add rt mean of incorrect trials
  result = c(result, mean(block_df[block_df$isAnsCorrect == 0,]$rt))
  # Get face inversion effect results
  face_inversion_values = getFaceInversionMean(block_df)
  # Add face inversion value
  result = c(result, face_inversion_values[1])
  # Add straight faces mean rt
  result = c(result, face_inversion_values[2])
  # Add reverted faces mean rt
  result = c(result, face_inversion_values[3])
  # Add rt mean log value
  result = c(result, log(mean(current_df$rt)))
  # Add rt SD log value
  result = c(result, log(sd(current_df$rt)))
  # Get log face inversion values
  face_inversion_log_values = getFaceInversionMean(current_df, T)
  # Add log face inversion value
  result = c(result, face_inversion_values[1])
  # Add log straight faces mean rt
  result = c(result, face_inversion_values[2])
  # Add log revert faces mean rt
  result = c(result, face_inversion_values[3])
  # Return result vector
  return(result)
}
```

```{r}
# Get online id and age from data frame, return it in vector
getOnlineIdAndAge = function(single_df) {
  # Create subset of single_df called text_df which contains only
  # survey-text trials
  text_df = single_df[single_df$trial_type == "survey-text", ]
  # Create online_id variable
  online_id = 0
  # Create age variable
  age = 0
  # For each row in text_df
  for(i in 1:nrow(text_df)) {
    # If there is a response
    if(text_df[i, "responses"] != "") {
      # Parse response to object called value
      value = fromJSON(text_df[i, "responses"])
      # If response length bigger than 2 its the online_id
      if(nchar(value$Q0) > 2) {
        # set online_id variable
        online_id = value$Q0
      } else {
        # Set age variable
        age = value$Q0
      }
    } 
  }
  # return online_id and age in vector
  return(c(online_id, age))
}
```

```{r}
# Get results vector of all blocks from single_df data frame
getAllBlocksValue = function(single_df, subject_id) {
  # Init of result vector
  result = c()
  # For each block in blockNames vector
  for(block in blockNames) {
    # Add the output vector of getAllValuesPerBlock of the current block
    # to the results vector
    result = c(result, getAllValuesPerBlock(single_df, block, subject_id))
  }
  # Return result vector
  return(result)
}
```

```{r}
# Get result vector of single subject, get data frame and subject id
getSingleSubject = function(df, subject_id) {
  # Get subset of df called current_subject_df contains all trials of
  # the given subject
  current_subject_df = df[df$subject == subject_id,]
  # Init result row
  row = c()
  # Add subject ID
  row = c(row, subject_id)
  # Add online ID and age
  row = c(row, getOnlineIdAndAge(current_subject_df))
  # Add general accuracy
  row = c(row, getAccResult(current_subject_df))
  # Add general accuracy without control
  row = c(row, getAccResult(current_subject_df, T))
  # Add general missed results
  row = c(row, getMissedResult(current_subject_df))
  # Add all other values returned in getAllBlocksValue
  row = c(row, getAllBlocksValue(current_subject_df, subject_id))
  # Return subject results in the vector row
  return(row)  
}
```

```{r}
# Create results data frame
getResults = function(df) {
  # Create cols vector
  cols = c("subject", "online_id", "age", "accuracy", "accuracy_no_control",
           "missed_trials", 
           "control_rt_mean", "control_rt_sd", "control_acc",
           "control_missed", "control_rt_mean_incorrect", 
           "control_face_inversion", "control_up_rt_mean",
           "control_down_rt_mean", "control_log_rt_mean", 
           "control_log_rt_sd", "control_log_face_inversion", 
           "control_log_up_rt_mean", "control_log_down_rt_mean",
           
           "10Where_rt_mean", "10Where_rt_sd", "10Where_acc",
           "10Where_missed", "10Where_rt_mean_incorrect",
           "10Where_face_inversion", "10Where_up_rt_mean",
           "10Where_down_rt_mean", "10Where_log_rt_mean", 
           "10Where_log_rt_sd", "10Where_log_face_inversion", 
           "10Where_log_up_rt_mean", "10Where_log_down_rt_mean",
           
           "6Where_rt_mean", "6Where_rt_sd", "6Where_acc", "6Where_missed", 
           "6Where_rt_mean_incorrect",  "6Where_face_inversion",
           "6Where_up_rt_mean", "6Where_down_rt_mean", 
           "6Where_log_rt_mean", "6Where_log_rt_sd", 
           "6Where_log_face_inversion", "6Where_log_up_rt_mean", 
           "6Where_log_down_rt_mean",
           
           "6What_rt_mean", "6What_rt_sd", "6What_acc", "6What_missed", 
           "6What_rt_mean_incorrect", "6What_face_inversion", 
           "6What_up_rt_mean", "6What_down_rt_mean", "6What_log_rt_mean", 
           "6What_log_rt_sd", "6What_log_face_inversion", 
           "6What_log_up_rt_mean", "6What_log_down_rt_mean",
           
           "10What_rt_mean", "10What_rt_sd", "10What_acc", "10What_missed",
           "10What_rt_mean_incorrect", "10What_face_inversion", 
           "10What_up_rt_mean", "10What_down_rt_mean", "10What_log_rt_mean",
           "10What_log_rt_sd", "10What_log_face_inversion", 
           "10What_log_up_rt_mean", "10What_log_down_rt_mean")
  # init results data frame  
  result_df = data.frame(matrix(NA, ncol=71))
  # Set columns names by cols vector
  colnames(result_df) = cols
  # Set row_count variable
  row_count = 0
  # For each subject
  for(subject in unique(df$subject)) {
    # Add subject row (result of getSingleSubject) to results data frame
    result_df[row_count + 1, ] = getSingleSubject(df, subject)
    # Increase row count by 1
    row_count = row_count + 1
  }
  # Return results data frame
  return(result_df)
}
```

```{r}
# Set results data frame in results_df variable
results_df = getResults(prev_df)
# Print row count of results df
print(nrow(results_df))
```

```{r}
# Init bad online ids vector
bad_online_ids = c()
# Init current_df variable
current_df = data.frame()
# For each subject
for(subject in unique(results_df$subject)) {
  # Check if results contains BPQ questionnaire, if not its mean the
  # subject did not finish the experiment and will be excluded
  current_df = prev_df[(prev_df$subject == subject) & 
                         (nchar(prev_df$responses) == 151),]
  # If BPQ does not exist
  if(nrow(current_df) == 0) {
    # Add his online_id to 
    bad_online_ids = c(bad_online_ids, 
                       results_df[results_df$subject == subject,]
                       [1,"online_id"])
  }
}
```

```{r}
# Bad online ids vector, as decided by the author
bad_online_ids = c(bad_online_ids, "5c239f96da51990001e21d97",
                   "614e028cce0c68ed60059ab1", "55b0029bfdf99b5c00619440",
                   "614e149fccc6a53ae06dfcd2", "5d227f901e99f80018c60e1f")

# Remove bad online ids
results_df = results_df[!(results_df$online_id %in% bad_online_ids),]

# Print results data frame row count
print(nrow(results_df))
```

```{r}
# Remove under threshold 0.75% accuracy
results_df = results_df[results_df$accuracy > 0.75, ]

# Print results data frame row count
print(nrow(results_df))
```

```{r}
# Set results_df to numeric
results_df[] <- lapply(results_df, function(x) as.numeric(x))

# Add scaled column for all blocks rt mean
results_df$scaled_control_rt_mean = scale(results_df$control_rt_mean)
results_df$scaled_6What_rt_mean = scale(results_df[["6What_rt_mean"]])
results_df$scaled_6Where_rt_mean = scale(results_df[["6Where_rt_mean"]])
results_df$scaled_10what_rt_mean = scale(results_df[["10What_rt_mean"]])
results_df$scaled_10where_rt_mean = scale(results_df[["10Where_rt_mean"]])

# Remove subject with bigger or smaller by 3 SD from mean in each condition
results_df = results_df[abs(results_df$scaled_control_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_6What_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_6Where_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_10what_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_10where_rt_mean) < 3, ]

# Print results data frame row count
print(nrow(results_df))
```

## 2.3 [80 words; 5 points]

### 2.3.1 

67 Subjects were left in the sample overall.

### 2.3.2

Mean of trials left in each condition after cleaning:
10 Hz, Where -- 57.79104
6 Hz, Where -- 56.80597
10 Hz, What -- 56.52239
6 Hz, What -- 54.89552
CONTROL (training) 10 Hz, Where -- 56.35821

```{r}
# Create data frame for compare in trial clean 
in_trial_clean_df = data.frame(matrix(NA, ncol=4))
# Set in_trial_clean_df_row_count to 0
in_trial_clean_df_row_count = 0
# Set columns names
colnames(in_trial_clean_df) = c("subject", "condition", "before", "after")
# For each subject
for(subject in unique(results_df$subject)) {
  # For each block
  for(block in blockNames) {
    # Create current row with subject and block
    in_trial_row = c(subject, block)
    # Create data frame of all trials in relevant subject and block
    b_df = prev_df[(prev_df$subject == subject) & 
                         (prev_df$stimulus_block == block),]  
    # Add row count of before data clean to in_trial_row
    in_trial_row = c(in_trial_row, nrow(b_df))
    # Add row count of after data clean to in_trial_row
    in_trial_row = c(in_trial_row, nrow(getCleanInTrialDf(b_df)))
    # Add in_trial_row to in_trial_clean_df
    in_trial_clean_df[in_trial_clean_df_row_count + 1, ] = in_trial_row
    # increase in_trial_clean_df_row_count by 1
    in_trial_clean_df_row_count = in_trial_clean_df_row_count + 1
  }
}
```

```{r}
# Set after column to numeric
in_trial_clean_df$after = as.numeric(in_trial_clean_df$after)

# calculating the mean amount of trials left after cleaning
print("10Where")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "10Where"]))
print("6Where")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "6Where"]))
print("10What")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "10What"]))
print("6What")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "6What"]))
print("10controlWhere")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == 
                                                "10controlWhere"]))
```


### 2.3.3

Not longitudinal data, so not a relevant question.

### 2.3.4

the distribution of number of trials/time-points included by participant, as can 
be seen by the histograms below, is a right-skewed distribution.

```{r}
# Create histogram for trials count after clean for 10Where condition
p_where_10 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "10Where",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: Where, Hz: 10 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_where_10
# Create histogram for trials count after clean for 10What condition
p_what_10 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "10What",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: What, Hz: 10 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_what_10
# Create histogram for trials count after clean for 6Where condition
p_where_6 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "6Where",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: Where, Hz: 6 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_where_6
# Create histogram for trials count after clean for 6What condition
p_what_6 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "6What",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: What, Hz: 6 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_what_6
# Create histogram for trials count after clean for 10controlWhere condition
p_control = ggplot(in_trial_clean_df[in_trial_clean_df$condition == 
                                       "10controlWhere",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: Control Where, Hz: 10 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_control
```


## 2.4 [100 words; 4 points]

Several statistical tests were performed. First, the correlations of the RT were checked 
for each block and thus for each condition (10Hz - where, 6Hz - where, 10Hz - 
what, 6Hz what). The analysis was done using a correlation table function 
(without corrections for multiple comparisons).
Second, two paired t-tests were conducted - one comparing the RT of all the what 
trials versus the where trials (ignoring the Hz conditions), and the other 
comparing the RT of all the 6HZ trials versus the 10HZ trials (ignoring the task
- what / where).
Finally, a two-way repeated measures ANOVA was performed, in a 2x2 array.


## 2.5 [150 words; 10 points]

You can see the a priori statistical power, as well as the effect sizes (low, 
medium and high) for each of the tests done, in the table: "ppv_df". It is worth 
noting that even though two t-tests were done, the statistical power was only 
calculated for one t-test - since there was no different hypothesis, different 
sample size or different estimated effect size in the two tests. The same goes 
for correlations.

It seems that the PPV increases as the effect size and statistical power 
increases.
Apart from that, it seems that regularly the power of the test and the PPV of 
the ANOVA test come out smaller, and it might be due to multiple comparisons.
ANOVA tests involve comparing means between multiple groups, which may increase 
the likelihood of false positives (type I errors). This can lower the PPV of the 
test, even if the effect size is like that of the t-test.


```{r}
# Create ppv_df for ppv and power analysis storage
ppv_df = data.frame(matrix(NA, ncol = 4))

# Set columns of ppv_df
colnames(ppv_df) = c("test", "effect size", "power", "ppv")

# Set reasonable R (at least from our perspective) variable to 0.6
reasonable_R = 0.6

# Set significance level variable to 0.05
sig_level = 0.05

# Create calculate ppv function
calculate_ppv = function(R, power, a) {
  return((power * R) / ((power * R) + a))
}
```

```{r}
# Calculate power for high effect in correlation test
high_corr_power = pwr.r.test(n = 67, r = 0.6, sig.level = 0.05)
# Add to ppv_df
ppv_df[1, ] = c("corr", "high", high_corr_power$power, 
                calculate_ppv(reasonable_R, high_corr_power$power, sig_level))

# Calculate power for high effect in anova test
# Cohen f = Cohen d / 2
high_anova_power = wp.rmanova(n = 67, ng = 2, nm = 2, f = 0.3, 
                              alpha = 0.05, type = 1)
# Add to ppv_df
ppv_df[2, ] = c("anova", "high", high_anova_power$power, 
                calculate_ppv(reasonable_R, high_anova_power$power, sig_level))

# Calculate power for high effect in t test
high_t_power = pwr.t.test(n = 67, d = 0.6, sig.level = 0.05,
                               type = c("paired"))
# Add to ppv_df
ppv_df[3, ] = c("t", "high", high_t_power$power, 
                calculate_ppv(reasonable_R, high_t_power$power, sig_level))

# Calculate power for medium effect in correlation test
medium_corr_power = pwr.r.test(n = 67, r = 0.35, sig.level = 0.05)
# Add to ppv_df
ppv_df[4, ] = c("corr", "medium", medium_corr_power$power, 
                calculate_ppv(reasonable_R, medium_corr_power$power, sig_level))

# Calculate power for medium effect in anova test
# Cohen f = Cohen d / 2
medium_anova_power = wp.rmanova(n = 67, ng = 2, nm = 2, f = 0.175, 
                              alpha = 0.05, type = 1)
# Add to ppv_df
ppv_df[5, ] = c("anova", "medium", medium_anova_power$power, 
                calculate_ppv(reasonable_R, medium_anova_power$power, 
                              sig_level))

# Calculate power for medium effect in t test
medium_t_power = pwr.t.test(n = 67, d = 0.35, sig.level = 0.05,
                               type = c("paired"))
# Add to ppv_df
ppv_df[6, ] = c("t", "medium", medium_t_power$power, 
                calculate_ppv(reasonable_R, medium_t_power$power, sig_level))

# Calculate power for small effect in correlation test
small_corr_power = pwr.r.test(n = 67, r = 0.15, sig.level = 0.05)
# Add to ppv_df
ppv_df[7, ] = c("corr", "small", small_corr_power$power, 
                calculate_ppv(reasonable_R, small_corr_power$power, sig_level))

# Calculate power for small effect in anova test
# Cohen f = Cohen d / 2
small_anova_power = wp.rmanova(n = 67, ng = 2, nm = 2, f = 0.075, 
                              alpha = 0.05, type = 1)
# Add to ppv_df
ppv_df[8, ] = c("anova", "small", small_anova_power$power, 
                calculate_ppv(reasonable_R, small_anova_power$power, 
                              sig_level))

# Calculate power for small effect in t test
small_t_power = pwr.t.test(n = 67, d = 0.15, sig.level = 0.05,
                               type = c("paired"))
# Add to ppv_df
ppv_df[9, ] = c("t", "small", small_t_power$power, 
                calculate_ppv(reasonable_R, small_t_power$power, sig_level))

```

## 2.6 [250 words; 10 points]

In the correlation tests, there were indeed very strong positive relationships 
between the different experimental conditions significantly. The weakest 
relationship found is between the 10Hz-what and the 6Hz-what conditions (r = 
0.74, p-value = 1.083e-12), and in second place the weakest relationship was 
between the 10Hz-where and the 6Hz-what conditions. Although the nature of the
relationships is the same as those reported before, the estimates of r and 
p-values were not accurate to those reported.

The same happened with the ANOVA test and the t-test. In the ANOVA analysis, 
simple main effects analysis showed that both the Hz and the task did have a 
statistically significant effect on RTs (F = 95.52, p-value = 1.88e-14; F = 
109.6, p-value = 1.17e-15, respectively). The interaction effect was not 
significant at all (p-value = 0.723). These are the same results previously 
reported: the main effects were significant, and the interaction effect was not. 
However, the numbers were slightly different. Even in the t-test, the nature of 
the results is the same and the numbers are slightly different. The RT of what 
and 6Hz is significantly greater than the RT of where and 10Hz, respectively.

It seems then that in general we reproduced the results, although not precisely. 
The main explanation for this is probably that the analyzes were originally done 
in matlab, and we have now done it in R.
R and MATLAB may have different default options for conducting statistical 
tests, for example – use different algorithms which can result in differences 
in precision, have different versions or implementation of the ANOVA function, 
the p-values probably was rounded due to the small number. Nevertheless, the 
essence of the results were replicated.


```{r}
# Create analysis results data frame
analysis_results_df = data.frame(matrix(NA, ncol = 7))
# Change columns of analysis_results_df
colnames(analysis_results_df) = c("index", "description", "type", 
                                  "effect_size", "p_value", 
                                  "CI_low", "CI_high")
```


### Analysis 1

Table: 6, 10 vs what, where

```{r}
# Create subset contains only the rt log means
analysis_1 = results_df[, c("6Where_rt_mean", "10Where_rt_mean",
                       "6What_rt_mean", "10What_rt_mean"),]

# Change columns name to prettier
colnames(analysis_1) = c("6Where", "10Where", "6What", "10What")

# calculate cor test of 6Where 6What
cor_value = cor.test(analysis_1[["6Where"]], analysis_1[["6What"]])
# Add to analysis_results_df
analysis_results_df[1, ] = c("1", "6Where 6What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 6Where 10What
cor_value = cor.test(analysis_1[["6Where"]], analysis_1[["10What"]])
# Add to analysis_results_df
analysis_results_df[2, ] = c("1", "6Where 10What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10Where 10What
cor_value = cor.test(analysis_1[["10Where"]], analysis_1[["10What"]])
# Add to analysis_results_df
analysis_results_df[3, ] = c("1", "10Where 10What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10Where 6What
cor_value = cor.test(analysis_1[["10Where"]], analysis_1[["6What"]])
# Add to analysis_results_df
analysis_results_df[4, ] = c("1", "10Where 6What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10Where 6Where
cor_value = cor.test(analysis_1[["10Where"]], analysis_1[["6Where"]])
# Add to analysis_results_df
analysis_results_df[5, ] = c("1", "10Where 6Where corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10What 6What
cor_value = cor.test(analysis_1[["10What"]], analysis_1[["6What"]])
# Add to analysis_results_df
analysis_results_df[6, ] = c("1", "10What 6What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
```

```{r}
# Create corr plot
corrplot(cor(analysis_1), p.mat = cor.mtest(analysis_1, conf.level = 0.95)$p, 
         insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05))
```

### Analysis 2

```{r}
### preparing proper DF for RM ANOVA ###

# Create ANOVA df
anova_df = results_df
anova_df$BT_where_6Hz = results_df[["6Where_rt_mean"]]
anova_df$BT_where_10Hz = results_df[["10Where_rt_mean"]]
anova_df$BT_what_6Hz = results_df[["6What_rt_mean"]]
anova_df$BT_what_10Hz = results_df[["10What_rt_mean"]]

anova_df = anova_df[, c("subject", "BT_where_6Hz", "BT_where_10Hz", 
                        "BT_what_6Hz", "BT_what_10Hz")]

# Wide to long of ANOVA df
LongAnova_df <- wideToLong(data = anova_df, 
                           within = c("task", "hz"))

# Set factors and numeric to the relevant columns
LongAnova_df <- data.frame(subject = as.factor(x = LongAnova_df$subject),
                            task = as.factor(x = LongAnova_df$task),
                            hz = as.factor(x = LongAnova_df$hz),
                            BT = as.numeric(LongAnova_df$BT))

### analysis ###

# Create two way anova
twoWayAnovaRM <- aov(formula = BT ~ (task * hz) + 
                       Error(subject/(task * hz)),
                     data = LongAnova_df)

# Print model results
print(x = model.tables(x = twoWayAnovaRM,
                       type = "mean"))

# Summery Model
summary(twoWayAnovaRM)

# Calculate anova effect size
anova_effect_size = eta_squared(twoWayAnovaRM, partial = T, alternative = "two")
# Add to analysis_results_df
for(i in 1:nrow(anova_effect_size)) {
  analysis_results_df[6 + i, ] = c("2", anova_effect_size[i, "Parameter"], 
                                   "anova", 
                                   anova_effect_size[i, "eta.sq.part"],
                                   anova_effect_size[i, "p"], 
                                   anova_effect_size$CI_low, 
                                   anova_effect_size$CI_high)
}

# Interaction graph
lineplot.CI(x.factor = LongAnova_df$task,
            response = LongAnova_df$BT,
            group = LongAnova_df$hz,
            col = c("chocolate1","chocolate4"),
            xlab = "task",
            ylab = "BT")

```

### Analysis 3

```{r}
# Create subset with 2 columns, all the what and all the where
analysis_t_task = data.frame(what = c(results_df[["6What_rt_mean"]],
                                      results_df[["10What_rt_mean"]]),
                             where = c(results_df[["6Where_rt_mean"]],
                                       results_df[["10Where_rt_mean"]]))

# T test for what and where
t_test_task = t.test(analysis_t_task$what, 
                           analysis_t_task$where, 
                           var.equal=TRUE,
                           paired = TRUE)

# Add to analysis_results_df
analysis_results_df[10, ] = c("3", "t test task", "t", 
                              cohensD(analysis_t_task$what,
                                      analysis_t_task$where,
                                      method = "paired"), 
                              t_test_task$p.value,
                              t_test_task$conf.int[1],
                              t_test_task$conf.int[2])
t_test_task


boxplot(formula = LongAnova_df$BT ~ LongAnova_df$task,
xlab = "Task",
border = "seagreen4",
frame.plot = TRUE
)

boxplot(formula = LongAnova_df$BT ~ LongAnova_df$hz,
xlab = "Presentation",
border = "purple",
frame.plot = TRUE
)
```



### Analysis 4

```{r}
# Create subset with 2 columns, all the 6 Hz and all the 10 Hz
analysis_t_hz = data.frame(six = c(results_df[["6What_rt_mean"]],
                                        results_df[["6Where_rt_mean"]]),
                                ten = c(results_df[["10What_rt_mean"]],
                                      results_df[["10Where_rt_mean"]]))
# Do t test
t_test_hz = t.test(analysis_t_hz$six, analysis_t_hz$ten, 
                   var.equal=TRUE, paired=T)

# Add to analysis_results_df
analysis_results_df[11, ] = c("3", "t test hz", "t", 
                                   cohensD(analysis_t_hz$six, 
                                           analysis_t_hz$ten,
                                           method = "paired"),
                              t_test_hz$p.value,
                              t_test_hz$conf.int[1],
                              t_test_hz$conf.int[2])
```


## 2.7 [250 words; 8 points]

No effect sizes or confidence intervals were reported in the original study.
For the three types of statistical tests, confidence intervals as well as effect 
sizes were calculated in an adapted manner for each test (all of those estimates 
are shown in the "analysis_results_df"). 

The confidence interval provides a range of plausible values for the population 
statistic / effect size, which can help to assess the statistical significance 
of the difference, connection or interaction between groups.

For Pearson correlation, an appropriate effect size estimate to include would be 
the correlation coefficient, r. It provides a measure of the strength and 
direction of the linear relationship between two continuous variables.

For paired t-tests, an appropriate effect size estimate to include would be 
Cohen's d, which provides a standardized measure of the difference between two 
means, which can be useful for comparing effect sizes across different studies 
(or a replication of a study).

For a two-way repeated measures ANOVA, an appropriate effect size estimate to 
include would be partial eta squared (ηp²). Partial eta squared measures the 
proportion of variance in the dependent variable accounted for by each factor 
and their interaction, while accounting for the effects of other factors in the 
model. The effect size estimate can help determine the practical significance of 
the results and inform decisions about the importance of the factors in the 
model for future studies.

```{r}
analysis_results_df
```


# Part 3: What could be done differently? [48 points overall]

## 3.1 [200 words; 10 points]

In the following lines of code, we will perform the cleaning of the data from
our point of view, compared to the cleaning performed before. We would like to 
point out a few issues in the pre-processing ans analysis that were conducted:
*  The accuracy is measured for all conditions together and not per condition. 
In our cleaning, we will measure according to condition, use the accuracy rate 
used in the basic paradigm (90%; for 10Hz-where task) and lower the percentage 
of accuracy required as the difficulty increases in each condition.
*  The data was also cleaned together with the training trials (weighted for the 
percentage of accuracy for example), when in principle they should not be 
considered in the analysis. We made sure to filter it beforehand.
*  There was no use of LOG transformation on the RT during the analysis, and 
there was no effect-coding but regular coding (0 and 1) of the categorical 
variables.
*  There was an assumption of equality of variances in the t-test, when there is 
no basis for this assumption.
*  The ANOVA analysis did not include the categorical variable of face inverted/
straight as a control variable. Becausea robust effect such that RTs are higher 
for inverted faces is known to exist, it is very possible that this variable was 
noise that attenuated the results (perhaps even the interaction).


```{r}
# Load data file
new_df = read.csv("data.csv")
```

```{r}
# Change 5Where and 5What to 6Where and 6What
new_df$stimulus_block[new_df$stimulus_block == "5Where"] = "6Where"
new_df$stimulus_block[new_df$stimulus_block == "5What"] = "6What"
# Create blocknames vector
blockNamesWithoutControl = c('10Where', '6Where','6What', '10What')
```

```{r}
# Fix subject and add correctness column
new_df = getSubjectAndCorrectness(new_df)
```

```{r}
# Function that clean trials by distance from SD
getCleanedInTrialNewDf = function(df, sd_num=3) {
  # Create result data frame
  return_df = data.frame(matrix(NA, ncol=5))
  # Set first indicator
  first = T
  # Set columns names
  colnames(return_df) = c("subject","rt", "task", "hz", "inverted")
  # For each subject                     
  for(subject in unique(df$subject)) {
    # For each hz option
    for(single_hz in unique(df$hz)) {
      # for each task option
      for(single_task in unique(df$task)) {
        # get relevant data frame
        temp_df = df[(df$subject == subject) &
                      (df$hz == single_hz) & 
                      (df$task == single_task), ]
        # scale rt
        temp_df$scaled_rt = scale(temp_df$rt)
        # Remove trials with distance from mean bigger then sd_num
        temp_df = temp_df[abs(temp_df$scaled_rt) < sd_num, ]
        # Add to return df
        if(first) {
          return_df = temp_df[, c("subject","rt", "task", 
                                      "hz", "inverted")]
          first = F
        } else {
          return_df = rbind(return_df, 
                          temp_df[, c("subject","rt", "task", 
                                      "hz", "inverted")])
        }
      }
    }
  }
  # return return_df
  return(return_df)
}
```

```{r}
# Get results before between subject clean
old_results_df = getResults(prev_df)
# Get only the subjects with bad_online_id as authors set
old_results_df = old_results_df[old_results_df$online_id 
                                %in% bad_online_ids,]
# Get theirs subject ID's
new_bad_subjects = c(unique(old_results_df$subject))
```

```{r}
# Get subjects that should be removed due to accuracy problem
getNewRemovedSubject = function(df) {
  # Init vector for bad subjects
  remove_subjects = c()
  # For each subject
  for(subject in unique(df$subject)) {
    # Get each condition data frame
    subject_df = df[df$subject == subject, ]
    where_10_df = subject_df[(subject_df$task == "Where") & 
                               (subject_df$hz == "10"), ]
    where_6_df = subject_df[(subject_df$task == "Where") & 
                              (subject_df$hz == "6"), ]
    what_10_df = subject_df[(subject_df$task == "What") & 
                              (subject_df$hz == "10"), ]
    what_6_df = subject_df[(subject_df$task == "What") & 
                             (subject_df$hz == "6"), ]
    # Get each condition accuracy
    where_10_accuracy = getAccResult(where_10_df)
    where_6_accuracy = getAccResult(where_6_df)
    what_10_accuracy = getAccResult(what_10_df)
    what_6_accuracy = getAccResult(what_6_df)
    # If subject not fit the requirements add it to remove_subjects vector
    if ((where_10_accuracy < 0.9) || 
        (where_6_accuracy < 0.85) || 
        (what_10_accuracy < 0.85) || 
        (what_6_accuracy < 0.8)) {
      remove_subjects = c(remove_subjects, subject)
    }
  }
  # return remove_subjects
  return(remove_subjects)
}
```

```{r}
# Get new results data frame
getNewResult = function(bad_subject_list = c()) {
  # Create subject with "subject","rt", "stimulus_block", "stimulus", "isAnsCorrect" columns
  new_results_df = new_df[, c("subject","rt", "stimulus_block", "stimulus", "isAnsCorrect")]
  # Get only relevant trials (with block in blockNamesWithoutControl)
  new_results_df = new_results_df[
    new_results_df$stimulus_block %in% blockNamesWithoutControl, ]
  
  # Create hz column
  new_results_df$hz = apply(new_results_df, 1, 
                            FUN = function(row) {
    if(startsWith(row["stimulus_block"], "10"))
      return("10") else return("6")}) 
  
  # Create subject column
  new_results_df$task = apply(new_results_df, 1, FUN = function(row) {
    if(endsWith(row["stimulus_block"], "Where")) return("Where") 
    else return("What")
  }) 
  
  # Create inverted column
  new_results_df$inverted = apply(new_results_df, 1, FUN = function(row)
    if(startsWith(row["stimulus"], "f")) "straight" else "inverted")
  
  # Remove bad subjects as set by the authors
  new_results_df = new_results_df[
      !(new_results_df$subject %in%
          bad_subject_list),]
  
  # Get subjects with accuracy problem
  remove_subjects = getNewRemovedSubject(new_results_df)
  
  # Remove subjects with accuracy problem
  new_results_df = new_results_df[
      !(new_results_df$subject %in%
          remove_subjects),]
  
  # Get only correct answers
  new_results_df = new_results_df[new_results_df$isAnsCorrect == 1, ]
  
  # Get subset with columns: "subject","rt", "task", "hz", "inverted"
  new_results_df = new_results_df[, c("subject","rt", "task", 
                                      "hz", "inverted")]
  
  # Set rt to numeric
  new_results_df$rt = as.numeric(new_results_df$rt)
  
  # Return results data frame
  return(new_results_df)
}
```

```{r}
# Get results
new_results_df = getNewResult(new_bad_subjects)
# Clean results
clean_df = getCleanedInTrialNewDf(new_results_df)
```

```{r}
# Create data frame for mixed models
model_df = clean_df
# Set task as factor
model_df$task = as.factor(model_df$task)
# Set hz as factor
model_df$hz = as.factor(model_df$hz)
# Set inverted as factor
model_df$inverted = as.factor(model_df$inverted)

# Do effect coding for task
contrasts(model_df$task)[1] = -1
# Do effect coding for hz
contrasts(model_df$hz)[1] = -1
# Do effect coding for inverted
contrasts(model_df$inverted)[1] = -1

# Log rt
model_df$rt = log(model_df$rt)
```


## 3.2 [200 words; 8 points]

For the pre-processing of the data, we will perform the following 
transformations (that were conducted in the previous question): 

code-effect for the two independent variables: task (where/what) and 
presentation/HZ (6Hz/10Hz), as well as for the control variable - face inverted 
(yes or no). We will perform a LOG transformation on the dependent variable - RT.

For the model, as mentioned, we would like to predict the RT through the 
independent variables: task and HZ, as well as their interaction, and we would 
like to control for inverted faces, since the ANOVA model in the previous 
analysis did not take into account its proven effect on the RT. 

We will use the Maximal random effect structure. That is, we would like to add 
by-subject random intercepts and by-subject random slopes for the task 
condition, the hz condition, their interaction and the control variable of 
inverted faces.
If the model doesn't converge, we'll remove the random effect that explains the 
least variance, and, if required, repeat until convergence. 

```{r}
head(model_df)
```

## 3.3 [200 words; 10 points]

A linear mixed model was used to analyze the effects of task type (what / 
where), presentation (6HZ / 10HZ), and stimulus (face) inversion (as a control) 
on reaction time. The model included random intercepts and slopes for each 
subject, as well as fixed effects for task type, presentation, and stimulus 
inversion.

The results showed that presentation (6HZ) had a significant positive effect on 
reaction time (β = 0.12, t(61.9) = 13.1, p < .001), while task type (where) had 
a significant negative effect on reaction time (β = -0.07, t(61.98) = -8.14, p 
< .001), indicating that participants responded faster to the where task and 
10HZ presentation. Stimulus inversion had a significant negative effect (β = 
-0.05, t(61.97) = -10.79, p < .001) – participants responded faster to straight 
face. 

There was also a significant interaction between task type and presentation (β 
= 0.01, t(62.03) = 2.43, p = .018), suggesting that the effect of presentation 
on RT varied depending on the task.

These results provide evidence for the importance of both presentation frequency 
(hz) and task type of processing in influencing reaction time, and suggest that 
stimulus inversion may modulate the effect on reaction time.


```{r}
# Create mixed model
mixed_model = lmer(rt ~ task * hz + inverted + 
                          (1 + task * hz + inverted | subject), model_df)
# Present it in table
tab_model(mixed_model, show.stat = T,show.df = T)
```

```{r}
summary(mixed_model)
```

```{r}
# Plot mixed_model
plot(ggpredict(mixed_model, terms=c("task","hz", "inverted")))
```

## 3.4 [150 words; 10 points]

The effects discovered in the mixed-model in relation to task type and HZ on the 
RT, were already known to us following the ANOVA test. At the same time, the 
interaction between them was not significant in the previous analysis, so it is 
surprising for us that in the current model the interaction was found to be 
significant. Intuitively it would not have seemed to us that the interaction 
wouldn't be significant even in our model.

It is likely that controlling for face inversion contributed to the possibility 
of finding a significant interaction – since a significant effect of this 
variable on the RT was found in the model, and also the complex consideration 
that was not possible in the ANOVA for the individual difference may have 
contributed. It is known that in the bRMS task there are constant and 
particularly important individual differences, hence the importance of using 
random effects for this design and perhaps for its analysis.


## 3.5 [150 words; 10 points]

Unfortunately, we were unable to get the simulations to work before submission 
time (their runtime was exceptionally long), so we cannot answer this question 
in its entirety. However, we assume that the power will not be particularly 
high, because 63 participants (after the cleaning we conducted) for this kind of 
model is too small sample, and the amount of trials for each condition 
(especially when adding the categorical division of inverted face) is not 
sufficient. If so, if we were to conduct a replication, we would control that 
there are more than 60 trials for each condition - including inverted / straight 
face, and aspire to add subjects.

```{r}
summary(doTest(mixed_model, fixed("taskWhere", "z")))
```

```{r}
powerSim(mixed_model, fixed("taskWhere","z"), nsim=3000)
```

```{r}
summary(doTest(mixed_model, fixed("hz6", "z")))
```

```{r}
summary(powerSim(mixed_model,fixed("hz6","z"), nsim=3000))
```

