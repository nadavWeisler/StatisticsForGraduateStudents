---
title: "Final Project"
author: "Nadav Weisler - 316493758, Gaya Aran - 209636885"
output: html_document
---

```{r warning=FALSE}
require(rjson)
require(corrplot)
require(ez)
require(lsr)
require(DescTools)
require(sciplot)
require(pwr)
require(WebPower)
require(effectsize)
require(sjPlot)
require(lme4)
require(lmerTest)
require(ggplot2)
require(simr)
require(ggeffects)
```

```{r}
# setwd("SOME_PATH")
```

# Part 1: Describing the design and dataset [7 points overall]

## 1.1 [300 words overall; 7 points]

### 1.1.1 

The study's main research question was: how does different kinds of processing
("What" stream and "Where" stream), and the frequency of the stimulus 
presentation (10Hz and 6Hz), influence the breaking suppression time (BT/RT).

### 1.1.2

The study was experimental and had a within-subject design.

### 1.1.3 

The paradigm that was used in this study was bRMS (breaking Repeated Mask
Suppression; Abir & Hassin, 2020). The conditions in the task had a 2 (What/
Where) X 2 (10Hz/6Hz) design – which also represents the design of the entire
study.

In the standard "Where" paradigm of bRMS, participants are asked to indicate 
where the target stimulus is (left or right, in our case), while the masking is
changed mondrians. This study added a What condition, in which the participants
indicated if the target stimulus was a straight face or an inverted face, or in
other words – "What" is the target. Additionally, the masking itself was 
changed. Instead of using one mask between each presentation of the target
stimulus, two masks were used, changing the frequency of the stimulus 
presentation from the standard 10Hz to 6Hz.

### 1.1.4 

The study's dependent variable was breaking suppression time (BT), the 
independent variables were processing types ("what" and "where") and the 
frequency of the stimulus presentation (10Hz and 6Hz). A control variable (was
measured and controled it the design but not in the analysis) was face position
(inverted or straight).

### 1.1.5

The participants were recruited through Prolific, and were 100 American adults,
with no history of severe head trauma, native English speakers. Because of the
exploratory nature of the study, no power analysis was conducted and the sample
size was due to mainly convenience and the desire for a large enough sample.

### 1.1.6 

Another methodological detail you should be aware of is the face inversion 
effect – a well established effect that shows within subject BT's of straight 
faces trials will be shorter than BT's of inverted faces trials.

# Part 2: What was done already? [45 points overall]

## 2.1 [80 words; 4 points]

How was the data previously cleaned and pre-processed? Were any observations
(at the participant- or trial-level) removed? Based on what criteria? Were any transformations applied to variables? Briefly describe the processes applied, pertaining to these questions and others you deem relevant.

- At the participants level - there were 5 participant removed due to incomplication of the experiment, or because they gave the wrong ID in their Prolific 

## 2.2 [4 points]

Repeat the exact data cleaning procedure using your own code. Submit the code 
with detailed comments. 

```{r}
# Load data file
prev_df = read.csv("data.csv")
```

```{r}
# Fix 5Where and 5What to 6Where and 6What
prev_df$stimulus_block[prev_df$stimulus_block == "5Where"] = "6Where"
prev_df$stimulus_block[prev_df$stimulus_block == "5What"] = "6What"

# Create blocks name vector for future use
blockNames = c('10controlWhere', '10Where', '6Where','6What', '10What')
```

```{r}
# Function that fix subjects and correctness
getSubjectAndCorrectness = function(df) {
  # Current subject indicator
  currentSubject = 0
  # Create isAnsCorrect column
  df$isAnsCorrect = 0
  # For each row in data frame
  for(i in 1:nrow(df)) {
    # Fix subjects column
    # New subject started
    if(df[i, "trial_index"] == 0) {
      # Append 1 to currentSubject
      currentSubject = currentSubject + 1
    }
    # Set subject column to relevant subject
    df[i, "subject"] = currentSubject
    
    # Create isAnsCorrect column - If subject where correct in RMS task
    # If this trial is one of the RMS blocks
    if(is.element(df[i, "stimulus_block"], blockNames)) {
      # If its where condition
      if(endsWith(df[i, "stimulus_block"], "Where")) {
        # If it is correct (left side)
        if ((df[i, "key_press"] == "Q") & 
            (df[i, "stimulus_side"] == 1)) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is correct (right side) 
        else if ((df[i, "key_press"] == "P") & 
                 (df[i, "stimulus_side"] == 0)) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is missed (no response)
        else if (df[i, "key_press"] == -1){
          # Set -1 to isAnsCorrect
          df[i, "isAnsCorrect"] = -1
        } 
        # Else (wrong answer remains)
        else {
          # Set 0 to isAnsCorrect
          df[i, "isAnsCorrect"] = 0
        }
      } 
      # If its where condition
      else if(endsWith(df[i, "stimulus_block"], "What")) {
        # If it is correct (straight face)
        if ((df[i, "key_press"] == "Y") & 
            (startsWith(df[i, "stimulus"], "f"))) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is correct (inverted face)
        else if ((df[i, "key_press"] == "B") & 
                 (startsWith(df[i, "stimulus"], "d"))) {
          # Set 1 to isAnsCorrect
          df[i, "isAnsCorrect"] = 1
        } 
        # If it is missed (no response)
        else if (df[i, "key_press"] == -1){
          # Set -1 to isAnsCorrect
          df[i, "isAnsCorrect"] = -1
        } 
        # Else (wrong answer remains)
        else {
          # Set 0 to isAnsCorrect
          df[i, "isAnsCorrect"] = 0
        }
      }
    }
  }
  # Return fixed df with correctness
  return(df)
}
```

```{r}
# Update prev_df with valid subject and correctness column
prev_df = getSubjectAndCorrectness(prev_df)
```

```{r}
# Create getAccResult function, get data frame and indication if add
# control to the calculation, return Accuracy calculation that:
# Correct answers / all Answers
getAccResult = function(single_df, remove_control = F) {
  # If remove_control is true
  if(remove_control) {
    # Remove control trials from single_df
    single_df = single_df[single_df$stimulus_block != "10controlWhere",]
  } 
  # Get all incorrect and missed trials row count
  incorrect = nrow(single_df[(single_df$isAnsCorrect == 0) |
                             (single_df$isAnsCorrect == -1),])
  # Get all correct trials row count
  correct = nrow(single_df[single_df$isAnsCorrect == 1,])
  # Return accuracy result
  return((correct / (correct + incorrect)))
}
```

```{r}
# Get Missed results percent count get single_df and return missed
# results represent as missed trials count / all trial count
getMissedResult = function(single_df) {
    # Get all trials count
    all_rms_count = nrow(single_df[(single_df$isAnsCorrect == 0) | 
                                (single_df$isAnsCorrect == 1) |
                                (single_df$isAnsCorrect == -1), ])
    # Get missed trials count
    rms_misses = nrow(single_df[single_df$isAnsCorrect == -1, ])
    # return missed results
    return(rms_misses / all_rms_count)
}
```

```{r}
# Get Face inversion effect (mean of straight face minus mean of revert
# face) function, get data frame and Boolean log Return face inversion 
# effect, log values of log is true 
getFaceInversionMean = function(single_df, log=F) {
  # Get mean of rt when face is straight
  mainFaces = mean(single_df[startsWith(single_df$stimulus, "f"), ]$rt)
  # Get mean of rt when face is revert
  revertFaces = mean(single_df[startsWith(single_df$stimulus, "d"), ]$rt)
  # If log is true
  if(log) {
    # Return face inversion of log values, log rt of straight faces
    # and log rt of revert faces
    return(c(log10(mainFaces) - log10(revertFaces), 
             log(mainFaces), log10(revertFaces)))
  } else {
    # Return face inversion, rt of straight faces and rt of revert faces
    return(c(mainFaces - revertFaces, mainFaces, revertFaces))
  }
}
```

```{r}
# Get clean block data frame
getCleanInTrialDf = function(block_df) {
  # Only correct
  current_df = block_df[block_df$isAnsCorrect == 1, ]
  # Bigger than the threshold 200 ms
  current_df = current_df[current_df$rt > 200,]
  # Create scaled RT
  current_df$scaledRT = scale(current_df$rt)
  # Remove trials with rt bigger than mean +- 3 SD
  current_df = current_df[abs(current_df$scaledRT) < 3,]
  # Return clean data frame
  return(current_df)
}
```

```{r}
# Get all values for single block, get data frame and block name
# and return vector of all results for this block in this data frame
getAllValuesPerBlock = function(single_df, block_name, subject_id) {
  # Only in the relevant block
  block_df = single_df[single_df$stimulus_block == block_name,] 
  # Get clean block data frame
  current_df = getCleanInTrialDf(block_df)
  # Init results vector
  result = c()
  # Add rt mean
  result = c(result, mean(current_df$rt))
  # Add rt SD
  result = c(result, sd(current_df$rt))
  # Add accuracy
  result = c(result, getAccResult(block_df))
  # Add misses
  result = c(result, getMissedResult(block_df))
  # Add rt mean of incorrect trials
  result = c(result, mean(block_df[block_df$isAnsCorrect == 0,]$rt))
  # Get face inversion effect results
  face_inversion_values = getFaceInversionMean(block_df)
  # Add face inversion value
  result = c(result, face_inversion_values[1])
  # Add straight faces mean rt
  result = c(result, face_inversion_values[2])
  # Add reverted faces mean rt
  result = c(result, face_inversion_values[3])
  # Add rt mean log value
  result = c(result, log(mean(current_df$rt)))
  # Add rt SD log value
  result = c(result, log(sd(current_df$rt)))
  # Get log face inversion values
  face_inversion_log_values = getFaceInversionMean(current_df, T)
  # Add log face inversion value
  result = c(result, face_inversion_values[1])
  # Add log straight faces mean rt
  result = c(result, face_inversion_values[2])
  # Add log revert faces mean rt
  result = c(result, face_inversion_values[3])
  # Return result vector
  return(result)
}
```

```{r}
# Get online id and age from data frame, return it in vector
getOnlineIdAndAge = function(single_df) {
  # Create subset of single_df called text_df which contains only
  # survey-text trials
  text_df = single_df[single_df$trial_type == "survey-text", ]
  # Create online_id variable
  online_id = 0
  # Create age variable
  age = 0
  # For each row in text_df
  for(i in 1:nrow(text_df)) {
    # If there is a response
    if(text_df[i, "responses"] != "") {
      # Parse response to object called value
      value = fromJSON(text_df[i, "responses"])
      # If response length bigger than 2 its the online_id
      if(nchar(value$Q0) > 2) {
        # set online_id variable
        online_id = value$Q0
      } else {
        # Set age variable
        age = value$Q0
      }
    } 
  }
  # return online_id and age in vector
  return(c(online_id, age))
}
```

```{r}
# Get results vector of all blocks from single_df data frame
getAllBlocksValue = function(single_df, subject_id) {
  # Init of result vector
  result = c()
  # For each block in blockNames vector
  for(block in blockNames) {
    # Add the output vector of getAllValuesPerBlock of the current block
    # to the results vector
    result = c(result, getAllValuesPerBlock(single_df, block, subject_id))
  }
  # Return result vector
  return(result)
}
```

```{r}
# Get result vector of single subject, get data frame and subject id
getSingleSubject = function(df, subject_id) {
  # Get subset of df called current_subject_df contains all trials of
  # the given subject
  current_subject_df = df[df$subject == subject_id,]
  # Init result row
  row = c()
  # Add subject ID
  row = c(row, subject_id)
  # Add online ID and age
  row = c(row, getOnlineIdAndAge(current_subject_df))
  # Add general accuracy
  row = c(row, getAccResult(current_subject_df))
  # Add general accuracy without control
  row = c(row, getAccResult(current_subject_df, T))
  # Add general missed results
  row = c(row, getMissedResult(current_subject_df))
  # Add all other values returned in getAllBlocksValue
  row = c(row, getAllBlocksValue(current_subject_df, subject_id))
  # Return subject results in the vector row
  return(row)  
}
```

```{r}
# Create results data frame
getResults = function(df) {
  # Create cols vector
  cols = c("subject", "online_id", "age", "accuracy", "accuracy_no_control",
           "missed_trials", 
           "control_rt_mean", "control_rt_sd", "control_acc",
           "control_missed", "control_rt_mean_incorrect", 
           "control_face_inversion", "control_up_rt_mean",
           "control_down_rt_mean", "control_log_rt_mean", 
           "control_log_rt_sd", "control_log_face_inversion", 
           "control_log_up_rt_mean", "control_log_down_rt_mean",
           
           "10Where_rt_mean", "10Where_rt_sd", "10Where_acc",
           "10Where_missed", "10Where_rt_mean_incorrect",
           "10Where_face_inversion", "10Where_up_rt_mean",
           "10Where_down_rt_mean", "10Where_log_rt_mean", 
           "10Where_log_rt_sd", "10Where_log_face_inversion", 
           "10Where_log_up_rt_mean", "10Where_log_down_rt_mean",
           
           "6Where_rt_mean", "6Where_rt_sd", "6Where_acc", "6Where_missed", 
           "6Where_rt_mean_incorrect",  "6Where_face_inversion",
           "6Where_up_rt_mean", "6Where_down_rt_mean", 
           "6Where_log_rt_mean", "6Where_log_rt_sd", 
           "6Where_log_face_inversion", "6Where_log_up_rt_mean", 
           "6Where_log_down_rt_mean",
           
           "6What_rt_mean", "6What_rt_sd", "6What_acc", "6What_missed", 
           "6What_rt_mean_incorrect", "6What_face_inversion", 
           "6What_up_rt_mean", "6What_down_rt_mean", "6What_log_rt_mean", 
           "6What_log_rt_sd", "6What_log_face_inversion", 
           "6What_log_up_rt_mean", "6What_log_down_rt_mean",
           
           "10What_rt_mean", "10What_rt_sd", "10What_acc", "10What_missed",
           "10What_rt_mean_incorrect", "10What_face_inversion", 
           "10What_up_rt_mean", "10What_down_rt_mean", "10What_log_rt_mean",
           "10What_log_rt_sd", "10What_log_face_inversion", 
           "10What_log_up_rt_mean", "10What_log_down_rt_mean")
  # init results data frame  
  result_df = data.frame(matrix(NA, ncol=71))
  # Set columns names by cols vector
  colnames(result_df) = cols
  # Set row_count variable
  row_count = 0
  # For each subject
  for(subject in unique(df$subject)) {
    # Add subject row (result of getSingleSubject) to results data frame
    result_df[row_count + 1, ] = getSingleSubject(df, subject)
    # Increase row count by 1
    row_count = row_count + 1
  }
  # Return results data frame
  return(result_df)
}
```

```{r}
# Set results data frame in results_df variable
results_df = getResults(prev_df)
# Print row count of results df
print(nrow(results_df))
```

```{r}
# Init bad online ids vector
bad_online_ids = c()
# Init current_df variable
current_df = data.frame()
# For each subject
for(subject in unique(results_df$subject)) {
  # Check if results contains BPQ questionnaire, if not its mean the
  # subject did not finish the experiment and will be excluded
  current_df = prev_df[(prev_df$subject == subject) & 
                         (nchar(prev_df$responses) == 151),]
  # If BPQ does not exist
  if(nrow(current_df) == 0) {
    # Add his online_id to 
    bad_online_ids = c(bad_online_ids, 
                       results_df[results_df$subject == subject,]
                       [1,"online_id"])
  }
}
```

```{r}
# Bad online ids vector, as decided by the author
bad_online_ids = c(bad_online_ids, "5c239f96da51990001e21d97",
                   "614e028cce0c68ed60059ab1", "55b0029bfdf99b5c00619440",
                   "614e149fccc6a53ae06dfcd2", "5d227f901e99f80018c60e1f")

# Remove bad online ids
results_df = results_df[!(results_df$online_id %in% bad_online_ids),]

# Print results data frame row count
print(nrow(results_df))
```

```{r}
# Remove under threshold 0.75% accuracy
results_df = results_df[results_df$accuracy > 0.75, ]

# Print results data frame row count
print(nrow(results_df))
```

```{r}
# Set results_df to numeric
results_df[] <- lapply(results_df, function(x) as.numeric(x))

# Add scaled column for all blocks rt mean
results_df$scaled_control_rt_mean = scale(results_df$control_rt_mean)
results_df$scaled_6What_rt_mean = scale(results_df[["6What_rt_mean"]])
results_df$scaled_6Where_rt_mean = scale(results_df[["6Where_rt_mean"]])
results_df$scaled_10what_rt_mean = scale(results_df[["10What_rt_mean"]])
results_df$scaled_10where_rt_mean = scale(results_df[["10Where_rt_mean"]])

# Remove subject with bigger or smaller by 3 SD from mean in each condition
results_df = results_df[abs(results_df$scaled_control_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_6What_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_6Where_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_10what_rt_mean) < 3, ]
results_df = results_df[abs(results_df$scaled_10where_rt_mean) < 3, ]

# Print results data frame row count
print(nrow(results_df))
```

## 2.3 [80 words; 5 points]

Inspect and describe the data set after the data cleaning and pre-processing 
procedure. Answer the following questions, and submit the lines of code that
helped you answer them 

### 2.3.1 

How many participants are left in the sample overall? 

67 Subjects

### 2.3.2

If trial-level task: How many trials are included per participant (and per 
condition, if applicable) on average?

```{r}
# Create data frame for compare in trial clean 
in_trial_clean_df = data.frame(matrix(NA, ncol=4))
# Set in_trial_clean_df_row_count to 0
in_trial_clean_df_row_count = 0
# Set columns names
colnames(in_trial_clean_df) = c("subject", "condition", "before", "after")
# For each subject
for(subject in unique(results_df$subject)) {
  # For each block
  for(block in blockNames) {
    # Create current row with subject and block
    in_trial_row = c(subject, block)
    # Create data frame of all trials in relevant subject and block
    b_df = prev_df[(prev_df$subject == subject) & 
                         (prev_df$stimulus_block == block),]  
    # Add row count of before data clean to in_trial_row
    in_trial_row = c(in_trial_row, nrow(b_df))
    # Add row count of after data clean to in_trial_row
    in_trial_row = c(in_trial_row, nrow(getCleanInTrialDf(b_df)))
    # Add in_trial_row to in_trial_clean_df
    in_trial_clean_df[in_trial_clean_df_row_count + 1, ] = in_trial_row
    # increase in_trial_clean_df_row_count by 1
    in_trial_clean_df_row_count = in_trial_clean_df_row_count + 1
  }
}
```

```{r}
# Set after column to numeric
in_trial_clean_df$after = as.numeric(in_trial_clean_df$after)

# calculating the mean amount of trials left after cleaning
print("10Where")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "10Where"]))
print("6Where")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "6Where"]))
print("10What")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "10What"]))
print("6What")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == "6What"]))
print("10controlWhere")
print(mean(in_trial_clean_df$after[in_trial_clean_df$condition == 
                                                "10controlWhere"]))
```


### 2.3.3

If longitudinal data: How many time-points are included per participant (and per condition, if applicable) on average?

Not longitudinal

### 2.3.4

What is the distribution of number of trials/time-points included by participant? Draw a histogram

```{r}
# Create histogram for trials count after clean for 10Where condition
p_where_10 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "10Where",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: Where, Hz: 10 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_where_10
# Create histogram for trials count after clean for 10What condition
p_what_10 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "10What",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: What, Hz: 10 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_what_10
# Create histogram for trials count after clean for 6Where condition
p_where_6 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "6Where",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: Where, Hz: 6 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_where_6
# Create histogram for trials count after clean for 6What condition
p_what_6 = ggplot(in_trial_clean_df[in_trial_clean_df$condition == "6What",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: What, Hz: 6 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_what_6
# Create histogram for trials count after clean for 10controlWhere condition
p_control = ggplot(in_trial_clean_df[in_trial_clean_df$condition == 
                                       "10controlWhere",], 
          aes(x=after)) + 
  geom_histogram(color="black", fill="white") +
  labs(title="Task: Control Where, Hz: 10 - trials after data clean",
       x="Trials count after clean", 
       y = "Count") 
p_control
```


## 2.4 [100 words; 4 points]

What was the statistical procedure previously employed on the data set to answer
the main theoretical question? Briefly describe the statistical test(s) in the context of the study.

ANOVA, t-test, correlation

## 2.5 [150 words; 10 points]

Given the sample size originally used, and the statistical procedure utilized, 
what was the study’s a priori statistical power3? Compute power for different population effect sizes that you deem as “small”, “medium”, and “large” in the context of your research domain. 
Given the power levels computed and the alpha used, and given what you see as a reasonable value for p(H1) / p(H0) odds (i.e., R), what is your estimate of the study’s PPV? For this section, you can assume that all test’s assumptions were met. You can use an off-the-shelf library for power calculations (e.g.,library(pwr)) or run a simulation. Submit the code and briefly describe your findings and their implications

```{r}
# Create ppv_df for ppv and power analysis storage
ppv_df = data.frame(matrix(NA, ncol = 4))

# Set columns of ppv_df
colnames(ppv_df) = c("test", "effect size", "power", "ppv")

# Set reasonable R (at least from our perspective) variable to 0.6
reasonable_R = 0.6

# Set significance level variable to 0.05
sig_level = 0.05

# Create calculate ppv function
calculate_ppv = function(R, power, a) {
  return((power * R) / ((power * R) + a))
}
```

```{r}
# Calculate power for high effect in correlation test
high_corr_power = pwr.r.test(n = 67, r = 0.6, sig.level = 0.05)
# Add to ppv_df
ppv_df[1, ] = c("corr", "high", high_corr_power$power, 
                calculate_ppv(reasonable_R, high_corr_power$power, sig_level))

# Calculate power for high effect in anova test
# Cohen f = Cohen d / 2
high_anova_power = wp.rmanova(n = 67, ng = 2, nm = 2, f = 0.3, 
                              alpha = 0.05, type = 1)
# Add to ppv_df
ppv_df[2, ] = c("anova", "high", high_anova_power$power, 
                calculate_ppv(reasonable_R, high_anova_power$power, sig_level))

# Calculate power for high effect in t test
high_t_power = pwr.t.test(n = 67, d = 0.6, sig.level = 0.05,
                               type = c("paired"))
# Add to ppv_df
ppv_df[3, ] = c("t", "high", high_t_power$power, 
                calculate_ppv(reasonable_R, high_t_power$power, sig_level))

# Calculate power for medium effect in correlation test
medium_corr_power = pwr.r.test(n = 67, r = 0.35, sig.level = 0.05)
# Add to ppv_df
ppv_df[4, ] = c("corr", "medium", medium_corr_power$power, 
                calculate_ppv(reasonable_R, medium_corr_power$power, sig_level))

# Calculate power for medium effect in anova test
# Cohen f = Cohen d / 2
medium_anova_power = wp.rmanova(n = 67, ng = 2, nm = 2, f = 0.175, 
                              alpha = 0.05, type = 1)
# Add to ppv_df
ppv_df[5, ] = c("anova", "medium", medium_anova_power$power, 
                calculate_ppv(reasonable_R, medium_anova_power$power, 
                              sig_level))

# Calculate power for medium effect in t test
medium_t_power = pwr.t.test(n = 67, d = 0.35, sig.level = 0.05,
                               type = c("paired"))
# Add to ppv_df
ppv_df[6, ] = c("t", "medium", medium_t_power$power, 
                calculate_ppv(reasonable_R, medium_t_power$power, sig_level))

# Calculate power for small effect in correlation test
small_corr_power = pwr.r.test(n = 67, r = 0.15, sig.level = 0.05)
# Add to ppv_df
ppv_df[7, ] = c("corr", "small", small_corr_power$power, 
                calculate_ppv(reasonable_R, small_corr_power$power, sig_level))

# Calculate power for small effect in anova test
# Cohen f = Cohen d / 2
small_anova_power = wp.rmanova(n = 67, ng = 2, nm = 2, f = 0.075, 
                              alpha = 0.05, type = 1)
# Add to ppv_df
ppv_df[8, ] = c("anova", "small", small_anova_power$power, 
                calculate_ppv(reasonable_R, small_anova_power$power, 
                              sig_level))

# Calculate power for small effect in t test
small_t_power = pwr.t.test(n = 67, d = 0.15, sig.level = 0.05,
                               type = c("paired"))
# Add to ppv_df
ppv_df[9, ] = c("t", "small", small_t_power$power, 
                calculate_ppv(reasonable_R, small_t_power$power, sig_level))

```

## 2.6 [250 words; 10 points]

Repeat the NHST statistical procedure originally conducted. Submit the code 
used. Summarize your findings (in your own words). You can (and are encouraged) to use a Table and a Figure (max 1 of each) to present the findings. Were you able to replicate the original findings? If not, discuss what might be the source of the differences.

```{r}
# Create analysis results data frame
analysis_results_df = data.frame(matrix(NA, ncol = 7))
# Change columns of analysis_results_df
colnames(analysis_results_df) = c("index", "description", "type", 
                                  "effect_size", "p_value", 
                                  "CI_low", "CI_high")
```


### Analysis 1

Table: 6, 10 vs what, where

```{r}
# Create subset contains only the rt log means
analysis_1 = results_df[, c("6Where_rt_mean", "10Where_rt_mean",
                       "6What_rt_mean", "10What_rt_mean"),]

# Change columns name to prettier
colnames(analysis_1) = c("6Where", "10Where", "6What", "10What")

# calculate cor test of 6Where 6What
cor_value = cor.test(analysis_1[["6Where"]], analysis_1[["6What"]])
# Add to analysis_results_df
analysis_results_df[1, ] = c("1", "6Where 6What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 6Where 10What
cor_value = cor.test(analysis_1[["6Where"]], analysis_1[["10What"]])
# Add to analysis_results_df
analysis_results_df[2, ] = c("1", "6Where 10What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10Where 10What
cor_value = cor.test(analysis_1[["10Where"]], analysis_1[["10What"]])
# Add to analysis_results_df
analysis_results_df[3, ] = c("1", "10Where 10What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10Where 6What
cor_value = cor.test(analysis_1[["10Where"]], analysis_1[["6What"]])
# Add to analysis_results_df
analysis_results_df[4, ] = c("1", "10Where 6What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10Where 6Where
cor_value = cor.test(analysis_1[["10Where"]], analysis_1[["6Where"]])
# Add to analysis_results_df
analysis_results_df[5, ] = c("1", "10Where 6Where corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
# calculate cor test of 10What 6What
cor_value = cor.test(analysis_1[["10What"]], analysis_1[["6What"]])
# Add to analysis_results_df
analysis_results_df[6, ] = c("1", "10What 6What corr", "corr",
  cor_value$estimate, cor_value$p.value, cor_value$conf.int[1],
  cor_value$conf.int[2])
```

```{r}
# Create corr plot
corrplot(cor(analysis_1), p.mat = cor.mtest(analysis_1, conf.level = 0.95)$p, 
         insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05))
```

### Analysis 2

```{r}
### preparing proper DF for RM ANOVA ###

# Create ANOVA df
anova_df = results_df
anova_df$BT_where_6Hz = results_df[["6Where_rt_mean"]]
anova_df$BT_where_10Hz = results_df[["10Where_rt_mean"]]
anova_df$BT_what_6Hz = results_df[["6What_rt_mean"]]
anova_df$BT_what_10Hz = results_df[["10What_rt_mean"]]

anova_df = anova_df[, c("subject", "BT_where_6Hz", "BT_where_10Hz", 
                        "BT_what_6Hz", "BT_what_10Hz")]

# Wide to long of ANOVA df
LongAnova_df <- wideToLong(data = anova_df, 
                           within = c("task", "hz"))

# Set factors and numeric to the relevant columns
LongAnova_df <- data.frame(subject = as.factor(x = LongAnova_df$subject),
                            task = as.factor(x = LongAnova_df$task),
                            hz = as.factor(x = LongAnova_df$hz),
                            BT = as.numeric(LongAnova_df$BT))

### analysis ###

# Create two way anova
twoWayAnovaRM <- aov(formula = BT ~ (task * hz) + 
                       Error(subject/(task * hz)),
                     data = LongAnova_df)

# Print model results
print(x = model.tables(x = twoWayAnovaRM,
                       type = "mean"))

# Summery Model
summary(twoWayAnovaRM)

# Calculate anova effect size
anova_effect_size = eta_squared(twoWayAnovaRM, partial = T, alternative = "two")
# Add to analysis_results_df
for(i in 1:nrow(anova_effect_size)) {
  analysis_results_df[6 + i, ] = c("2", anova_effect_size[i, "Parameter"], 
                                   "anova", 
                                   anova_effect_size[i, "eta.sq.part"],
                                   anova_effect_size[i, "p"], 
                                   anova_effect_size$CI_low, 
                                   anova_effect_size$CI_high)
}

# Interaction graph
lineplot.CI(x.factor = LongAnova_df$task,
            response = LongAnova_df$BT,
            group = LongAnova_df$hz,
            col = c("chocolate1","chocolate4"),
            xlab = "task",
            ylab = "BT")

```

### Analysis 3

```{r}
# Create subset with 2 columns, all the what and all the where
analysis_t_task = data.frame(what = c(results_df[["6What_rt_mean"]],
                                      results_df[["10What_rt_mean"]]),
                             where = c(results_df[["6Where_rt_mean"]],
                                       results_df[["10Where_rt_mean"]]))

# T test for what and where
t_test_task = t.test(analysis_t_task$what, 
                           analysis_t_task$where, 
                           var.equal=TRUE,
                           paired = TRUE)

# Add to analysis_results_df
analysis_results_df[10, ] = c("3", "t test task", "t", 
                              cohensD(analysis_t_task$what,
                                      analysis_t_task$where,
                                      method = "paired"), 
                              t_test_task$p.value,
                              t_test_task$conf.int[1],
                              t_test_task$conf.int[2])
t_test_task
```



### Analysis 4

```{r}
# Create subset with 2 columns, all the 6 Hz and all the 10 Hz
analysis_t_hz = data.frame(six = c(results_df[["6What_rt_mean"]],
                                        results_df[["6Where_rt_mean"]]),
                                ten = c(results_df[["10What_rt_mean"]],
                                      results_df[["10Where_rt_mean"]]))
# Do t test
t_test_hz = t.test(analysis_t_hz$six, analysis_t_hz$ten, 
                   var.equal=TRUE, paired=T)

# Add to analysis_results_df
analysis_results_df[11, ] = c("3", "t test hz", "t", 
                                   cohensD(analysis_t_hz$six, 
                                           analysis_t_hz$ten,
                                           method = "paired"),
                              t_test_hz$p.value,
                              t_test_hz$conf.int[1],
                              t_test_hz$conf.int[2])
```


## 2.7 [250 words; 8 points]

Were effect size estimates and/or confidence intervals originally computed / reported? If yes, re estimate them using your own code. Plus, discuss whether additional estimates are required in your opinion, and if so, compute them. If not, choose and justify what estimates to include, and write a script that computes them. In both cases: Submit the code used, discuss the computed estimates, the justification behind their use, and discuss the implications of the estimates

```{r}
analysis_results_df
```


# Part 3: What could be done differently? [48 points overall]

## 3.1 [200 words; 10 points]

Discuss – is the procedure previously utilized proper for the data set at hand? Were any assumptions potentially violated by using this procedure? Are there any additional sources of variance not being accounted for by the method used (e.g., ignored random effects or control variables?)? Any other methodological issues related to the employed test? Critically discuss the identified issues. In parallel, inspect the data and try to demonstrate evidence for the potential issue(s) you’ve identified. Submit this code. If relevant, accompany your description with a figure.


```{r}
# Load data file
new_df = read.csv("data.csv")
```

```{r}
# Change 5Where and 5What to 6Where and 6What
new_df$stimulus_block[new_df$stimulus_block == "5Where"] = "6Where"
new_df$stimulus_block[new_df$stimulus_block == "5What"] = "6What"
# Create blocknames vector
blockNamesWithoutControl = c('10Where', '6Where','6What', '10What')
```

```{r}
# Fix subject and add correctness column
new_df = getSubjectAndCorrectness(new_df)
```

```{r}
# Function that clean trials by distance from SD
getCleanedInTrialNewDf = function(df, sd_num=3) {
  # Create result data frame
  return_df = data.frame(matrix(NA, ncol=5))
  # Set first indicator
  first = T
  # Set columns names
  colnames(return_df) = c("subject","rt", "task", "hz", "inverted")
  # For each subject                     
  for(subject in unique(df$subject)) {
    # For each hz option
    for(single_hz in unique(df$hz)) {
      # for each task option
      for(single_task in unique(df$task)) {
        # get relevant data frame
        temp_df = df[(df$subject == subject) &
                      (df$hz == single_hz) & 
                      (df$task == single_task), ]
        # scale rt
        temp_df$scaled_rt = scale(temp_df$rt)
        # Remove trials with distance from mean bigger then sd_num
        temp_df = temp_df[abs(temp_df$scaled_rt) < sd_num, ]
        # Add to return df
        if(first) {
          return_df = temp_df[, c("subject","rt", "task", 
                                      "hz", "inverted")]
          first = F
        } else {
          return_df = rbind(return_df, 
                          temp_df[, c("subject","rt", "task", 
                                      "hz", "inverted")])
        }
      }
    }
  }
  # return return_df
  return(return_df)
}
```

```{r}
# Get results before between subject clean
old_results_df = getResults(prev_df)
# Get only the subjects with bad_online_id as authors set
old_results_df = old_results_df[old_results_df$online_id 
                                %in% bad_online_ids,]
# Get theirs subject ID's
new_bad_subjects = c(unique(old_results_df$subject))
```

```{r}
# Get subjects that should be removed due to accuracy problem
getNewRemovedSubject = function(df) {
  # Init vector for bad subjects
  remove_subjects = c()
  # For each subject
  for(subject in unique(df$subject)) {
    # Get each condition data frame
    subject_df = df[df$subject == subject, ]
    where_10_df = subject_df[(subject_df$task == "Where") & 
                               (subject_df$hz == "10"), ]
    where_6_df = subject_df[(subject_df$task == "Where") & 
                              (subject_df$hz == "6"), ]
    what_10_df = subject_df[(subject_df$task == "What") & 
                              (subject_df$hz == "10"), ]
    what_6_df = subject_df[(subject_df$task == "What") & 
                             (subject_df$hz == "6"), ]
    # Get each condition accuracy
    where_10_accuracy = getAccResult(where_10_df)
    where_6_accuracy = getAccResult(where_6_df)
    what_10_accuracy = getAccResult(what_10_df)
    what_6_accuracy = getAccResult(what_6_df)
    # If subject not fit the requirements add it to remove_subjects vector
    if ((where_10_accuracy < 0.9) || 
        (where_6_accuracy < 0.85) || 
        (what_10_accuracy < 0.85) || 
        (what_6_accuracy < 0.8)) {
      remove_subjects = c(remove_subjects, subject)
    }
  }
  # return remove_subjects
  return(remove_subjects)
}
```

```{r}
# Get new results data frame
getNewResult = function(bad_subject_list = c()) {
  # Create subject with "subject","rt", "stimulus_block", "stimulus", "isAnsCorrect" columns
  new_results_df = new_df[, c("subject","rt", "stimulus_block", "stimulus", "isAnsCorrect")]
  # Get only relevant trials (with block in blockNamesWithoutControl)
  new_results_df = new_results_df[
    new_results_df$stimulus_block %in% blockNamesWithoutControl, ]
  
  # Create hz column
  new_results_df$hz = apply(new_results_df, 1, 
                            FUN = function(row) {
    if(startsWith(row["stimulus_block"], "10"))
      return("10") else return("6")}) 
  
  # Create subject column
  new_results_df$task = apply(new_results_df, 1, FUN = function(row) {
    if(endsWith(row["stimulus_block"], "Where")) return("Where") 
    else return("What")
  }) 
  
  # Create inverted column
  new_results_df$inverted = apply(new_results_df, 1, FUN = function(row)
    if(startsWith(row["stimulus"], "f")) "straight" else "inverted")
  
  # Remove bad subjects as set by the authors
  new_results_df = new_results_df[
      !(new_results_df$subject %in%
          bad_subject_list),]
  
  # Get subjects with accuracy problem
  remove_subjects = getNewRemovedSubject(new_results_df)
  
  # Remove subjects with accuracy problem
  new_results_df = new_results_df[
      !(new_results_df$subject %in%
          remove_subjects),]
  
  # Get only correct answers
  new_results_df = new_results_df[new_results_df$isAnsCorrect == 1, ]
  
  # Get subset with columns: "subject","rt", "task", "hz", "inverted"
  new_results_df = new_results_df[, c("subject","rt", "task", 
                                      "hz", "inverted")]
  
  # Set rt to numeric
  new_results_df$rt = as.numeric(new_results_df$rt)
  
  # Return results data frame
  return(new_results_df)
}
```

```{r}
# Get results
new_results_df = getNewResult(new_bad_subjects)
# Clean results
clean_df = getCleanedInTrialNewDf(new_results_df)
```

```{r}
# Create data frame for mixed models
model_df = clean_df
# Set task as factor
model_df$task = as.factor(model_df$task)
# Set hz as factor
model_df$hz = as.factor(model_df$hz)
# Set inverted as factor
model_df$inverted = as.factor(model_df$inverted)

# Do effect coding for task
contrasts(model_df$task)[1] = -1
# Do effect coding for hz
contrasts(model_df$hz)[1] = -1
# Do effect coding for inverted
contrasts(model_df$inverted)[1] = -1

# Log rt
model_df$rt = log(model_df$rt)
```


## 3.2 [200 words; 8 points]

Identify (some type of) a mixed-effect model analysis that can be used to 
analyze the data and address at least one of the issues discussed in 3.1. 
Fully describe the planned analysis and all pre processing steps it requires
(e.g., transformations, coding, random effect determination method,
etc.). Write a brief ‘analysis’ section (as if you are reporting your 
analysis plans in a paper/preregistration), containing all these details.
Also submit the code that instantiates these preparatory steps on your data.
Your mixed-effect model should contain at least two predictors, and in any case should address the work’s theoretical question and the issues you’ve identified 
in 3.1 above. 

```{r}
head(model_df)
```

## 3.3 [200 words; 10 points]

Run the mixed-effect model. Submit the code used to run it. Describe your 
findings, as if you are reporting it in a paper. You can add a Table and/or a Figure (max. 1 of each).

```{r}
# Create mixed model
mixed_model = lmer(rt ~ task * hz + inverted + 
                          (1 + task * hz + inverted | subject), model_df)
# Present it in table
tab_model(mixed_model, show.stat = T,show.df = T)
```

```{r}
summary(mixed_model)
```

```{r}
# Plot mixed_model
plot(ggpredict(mixed_model, terms=c("task","hz", "inverted")))
```

## 3.4 [150 words; 10 points]

Discuss how the results of the mixed-effect analysis compare to those originally observed/reported (using the “classic” tests). Do these differences (or lack thereof) fit your expectations? Are they in line with general differences between the two analytical approaches? Discuss what may have contributed to differences/similarities you observe.

## 3.5 [150 words; 10 points]

Assuming that the population effect size is the one observed in the mixed-effect analysis of the current data set, conduct a prospective power analysis for a replication study. We encourage you to use the simR package to circumvent needing complex simulations for this section4. What is the power of a replication with an identical sample size (and all other design properties)? If you think this power is too low or too high: What is the sample size needed to result in what you see as reasonable power? Submit the code used to run the power analysis, and describe what you found. You can accompany your description with a Figure. Do these results change your data collection plans as you prepare for your next study?

```{r}
summary(doTest(mixed_model, fixed("taskWhere", "z")))
```

```{r}
powerSim(mixed_model, fixed("taskWhere","z"), nsim=3000)
```

```{r}
summary(doTest(mixed_model, fixed("hz6", "z")))
```

```{r}
summary(powerSim(mixed_model,fixed("hz6","z"), nsim=3000))
```

