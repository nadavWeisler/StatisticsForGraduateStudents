# Gaya Aran, 209636885; Nadav Weisler, 316493758


```{r warning=FALSE}
require(rmarkdown)
require(dplyr)
require(ggpubr)
```

```{r}
TARGET <- 0.5
NON_TARGET <- -0.5
```

# Part 1

## Question 1

```{r}
setwd("C:/Repos/Study/StatisticsForGraduateStudents/EX/ex1")
attentionDrivesEmotion <- read.csv("attention-drives-emotion-2019-exp1.csv")
```


## Question 2

Rated intensity of emotional response -> *reaction*
Rated emotional intensity -> *intensity*
Subject identity -> *id*
Valence of image rated -> *ValenceContrast*
Whether the image was a target or-non target -> *target*

## Question 3

```{r}
attentionDrivesEmotion$rating <- 
  rowMeans(attentionDrivesEmotion[, c('intensity', 'reaction')], na.rm = TRUE)
```

## Question 4

```{r}
participants_count_before <- length(unique(attentionDrivesEmotion$id)) 

attentionDrivesEmotion <- attentionDrivesEmotion %>% 
  filter((demand_intensity != 2) & (ValenceContrast != 0))

participants_removed <- 
  (participants_count_before - length(unique(attentionDrivesEmotion$id)))
```

According to the R code, 15 participants were removed. 

## Question 5

```{r}
attentionDrivesEmotion$z_rating <- with(attentionDrivesEmotion, scale(rating))
```

## Question 6

```{r}
attentionDrivesEmotion <- attentionDrivesEmotion %>% 
  filter(abs(z_rating) < 2)
```


## Question 7
```{r}
dfSummary <- attentionDrivesEmotion %>%
group_by(id, target) %>%
  summarise(
    mean = mean(rating),
    sd = sd(rating),
    count = length(rating)
  )

sum(dfSummary[dfSummary$target == TARGET,]$count)

```
160 is the total number of target trials left.

## Question 8

```{r}
gender_df <- read.csv("attention-drives-emotion-2019-exp1-subject-gender.csv")
gender_df <- gender_df[ , !(names(gender_df) %in% c("X"))]
uniqe_gender_names <- unique(gender_df$gender)
uniqe_gender_names

gender_df$gender[grepl("Trans", gender_df$gender)] <- "O"
gender_df$gender[grepl("f", gender_df$gender)] <- "F"
gender_df$gender[grepl("F", gender_df$gender)] <- "F"
gender_df$gender[!grepl("F", gender_df$gender) & 
                   !grepl("O", gender_df$gender)] <- "M"

length(gender_df$gender[gender_df$gender == "M"])
length(gender_df$gender[gender_df$gender == "F"])
length(gender_df$gender[gender_df$gender == "O"])
```

36 Males, 63 Females and 1 Other.



## Question 9
```{r}
dfSummary <- merge(dfSummary, gender_df,by="id")
```

```{r}
genderSummery <- dfSummary %>%
group_by(gender) %>%
  summarise(
    mean = mean(mean),
    sd = mean(sd, na.rm = T)
  )
genderSummery
```
## Question 10

```{r}
print(table(dfSummary$target))
print(table(dfSummary$id))
```


```{r}
# After error msg in paired t.test we noticed there was a participant (id = 76)
# without target trials (after trial clean). we exclude this participant because
# paired t.test required both kinds of trials (target and non-target) for each
# participant.
dfSummary <- dfSummary[dfSummary$id != 76,]

t_test_Q10 <- t.test(x = dfSummary$mean[dfSummary$target == TARGET], 
                     y = dfSummary$mean[dfSummary$target == NON_TARGET], 
                     alternative = "greater", paired = TRUE, var.equal = TRUE)
t_test_Q10
```
As it was predicted by the authors, the target stimuli found to have greater intensity rating than non-target stimuli (target: M = 3.55, SD = 2.13, non-target: M = 3.27, SD = 1.65). In other words, the intensity of emotion reported by the participant for the spatially cued objects compared to non-cued objects (targets and non-targets) was significantly greater (t(82) = 1.95, p-value = 0.027).

## Question 11 
```{r}
target <- dfSummary[dfSummary$target == TARGET, ]$mean
nontarget <- dfSummary[dfSummary$target == NON_TARGET, ]$mean
df_rating <- data.frame(target = target, nontarget = nontarget)

ggpaired(df_rating, cond1 = "target", cond2 = "nontarget",
    fill = "condition", palette = "jco")
```

# Part 2
```{r}
set.seed(6)
```

## Question 1
```{r}
t_test_sim <- function(M1, M2, S1, S2, N1, N2) {
  group_1 <- rnorm(N1, M1, S1)
  group_2 <- rnorm(N2, M2, S2)
  result <- t.test(group_1, group_2, alternative = "two.sided", var.equal = T)
  t_statistic <- result$statistic
  p_value <- result$p.value
#  degrees_freedom <- result$parameter
#  print(degrees_freedom)
  return(data.frame(t_statistic, p_value))
}
results <- t_test_sim(0, 0, 1, 1, 50, 50)
results
```
t-score = -2.27, p-value = 0.024, df = 98.

## Question 2

```{r message=FALSE, warning=FALSE}
resultsA <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsA) <- c("t_statistic", "p_value")
resultsB <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsB) <- c("t_statistic", "p_value")
resultsC <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsC) <- c("t_statistic", "p_value")
resultsD <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsD) <- c("t_statistic", "p_value")
resultsE <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsE) <- c("t_statistic", "p_value")
resultsF <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsF) <- c("t_statistic", "p_value")

for (i in 1:1000){
  resultsA[i,] <- t_test_sim(0, 0, 1, 1, 30, 30)
  resultsB[i,] <- t_test_sim(0, 0, 1, 3, 30, 30)
  resultsC[i,] <- t_test_sim(0, 0, 1, 1, 90, 90)
  resultsD[i,] <- t_test_sim(0.5, 0, 1, 1, 30, 30)
  resultsE[i,] <- t_test_sim(sqrt(5) * 0.5, 0, 1, 3, 30, 30)
  resultsF[i,] <- t_test_sim(0.5, 0, 1, 1, 90, 90)
}

```
```{r}
hist(resultsA$t_statistic)
hist(resultsA$p_value)
hist(resultsB$t_statistic)
hist(resultsB$p_value)
hist(resultsC$t_statistic)
hist(resultsC$p_value)
hist(resultsD$t_statistic)
hist(resultsD$p_value)
hist(resultsE$t_statistic)
hist(resultsE$p_value)
hist(resultsF$t_statistic)
hist(resultsF$p_value)
```

## Question 3
### A
The p-value in part 2 question 1 was: 0.024

### B
When Ho is correct (according to the mean and SD that were selected - as in "resultsA"), the distribution becomes close to be uniformic distribution. However, when the H1 is correct (for example - "resultsD") the distribution becomes more sided to the left, exponential distribution, hence - the more the p-value is getting bigger, so the probability of it is getting smaller.

### C
We haven't found much of a difference in the t-score distributions when the sample size increase (as in "resultsC" and "resultsF"), but the distribution of the p-value becomes more characterized to the distribution types we could identify in the previous section. 

That is, when the effect is present the probability of getting a significant p-value increases significantly, and when there is no effect the probability of getting a p-value between 0 and 1 becomes more uniform. For example, in the distribution of "resultsC" where there was no effect, it can be seen that about 80 simulations out of 1000 produced a p-value between 0.0 and 0.1. If we assume that 40 of these simulations produced a p-value between 0.0 and 0.05, those 40 simulations actually demonstrate cases where a type 1 error was made (the p-value came out smaller than 0.05, even though there is no effect). Indeed, the probability that our simulation will come out with a significant p-value out of the other simulations approaches 5% (80/1000).

On the other hand, the distribution of "resultsF" versus "resultsD" shows a kind of demonstration of the statistical power that increases as the sample increases, in the case where there is an effect. The probability of obtaining a significant p-value increases significantly as the sample size increases. Accordingly, so does the probability for type 2 error decreases.

### D
The distributions in which the assumption of equality of variances was violated are "resultsB" (which we will compare to "resultsA") and "resultsE" (which we will compare to "resultsD", although the averages are not equal but in both cases there is an effect and the sample is the same, hence the general of the distribution as we saw in the previous section will be similar).

We must admit that the distributions of the p-value did not show a difference only between cases where there was an assumption of equality of variances and cases where this assumption was violated. Differences can be found mainly in the t-distributions. If we compare these distributions to the original t distribution, which is characterized by degrees of freedom, it seems that in distributions where the variances are not equal, the degrees of freedom are larger, even though the sample is not larger (the distribution becomes higher and narrower).



## Question 4
### A
```{r}
t_test_sim_welch <- function(M1, M2, S1, S2, N1, N2) {
  group_1 <- rnorm(N1, M1, S1)
  group_2 <- rnorm(N2, M2, S2)
  result <- t.test(group_1, group_2, alternative = "two.sided")
  t_statistic <- result$statistic
  p_value <- result$p.value
#  degrees_freedom <- result$parameter
#  print(degrees_freedom)
  return(data.frame(t_statistic, p_value))
}

t_test_sim_welch(0, 0, 1, 1, 30, 30)
```

```{r message=FALSE, warning=FALSE}
resultsA_welch <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsA_welch) <- c("t_statistic", "p_value")
resultsB_welch <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsB_welch) <- c("t_statistic", "p_value")
resultsC_welch <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsC_welch) <- c("t_statistic", "p_value")
resultsD_welch <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsD_welch) <- c("t_statistic", "p_value")
resultsE_welch <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsE_welch) <- c("t_statistic", "p_value")
resultsF_welch <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsF_welch) <- c("t_statistic", "p_value")

for (i in 1:1000){
  resultsA_welch[i,] <- t_test_sim_welch(0, 0, 1, 1, 30, 30)
  resultsB_welch[i,] <- t_test_sim_welch(0, 0, 1, 3, 30, 30)
  resultsC_welch[i,] <- t_test_sim_welch(0, 0, 1, 1, 90, 90)
  resultsD_welch[i,] <- t_test_sim_welch(0.5, 0, 1, 1, 30, 30)
  resultsE_welch[i,] <- t_test_sim_welch(sqrt(5) * 0.5, 0, 1, 3, 30, 30)
  resultsF_welch[i,] <- t_test_sim_welch(0.5, 0, 1, 1, 90, 90)
}

```
### B

```{r message=FALSE, warning=FALSE}
t_test_sim_ourQuest <- function(M1, M2, S1, S2, N1, N2) {
  sample_1 <- rnorm(N1, M1, S1)
  sample_2 <- rnorm(N2, M2, S2)
  result <- t.test(sample_1, sample_2, alternative = "two.sided", 
                   var.equal = TRUE, paired = TRUE)
  t_statistic <- result$statistic
  p_value <- result$p.value
  return(data.frame(t_statistic, p_value))
}

```

```{r message=FALSE, warning=FALSE}
t_test_sim_gup_between_M <- function(M1, M2, S1, S2, N1, N2) {
  sample_1 <- rnorm(N1, M1, S1)
  sample_2 <- rnorm(N2, M2, S2)
  result <- t.test(sample_2-sample_1, alternative = "two.sided", var.equal = TRUE)
  print(result)
  t_statistic <- result$statistic
  p_value <- result$p.value
  return(data.frame(t_statistic, p_value))
}
```

```{r message=FALSE, warning=FALSE}
resultsA_ourQuest <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsA_ourQuest) <- c("t_statistic", "p_value")
resultsB_ourQuest <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsB_ourQuest) <- c("t_statistic", "p_value")
resultsC_ourQuest <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsC_ourQuest) <- c("t_statistic", "p_value")
resultsD_ourQuest <- data.frame(matrix(NA, ncol=2, nrow=1000))
colnames(resultsD_ourQuest) <- c("t_statistic", "p_value")

for (i in 1:1000){
  resultsA_ourQuest[i,] <- t_test_sim_gup_between_M(0, 0, 1, 1, 30, 30)
  resultsB_ourQuest[i,] <- t_test_sim_ourQuest(0, 0, 1, 1, 30, 30)
  resultsC_ourQuest[i,] <- t_test_sim_gup_between_M(10, 0, 1, 1, 90, 90)
  resultsD_ourQuest[i,] <- t_test_sim_ourQuest(10, 0, 1, 1, 90, 90)
}
```

```{r}
hist(resultsA_ourQuest$t_statistic)
hist(resultsA_ourQuest$p_value)
hist(resultsB_ourQuest$t_statistic)
hist(resultsB_ourQuest$p_value)
hist(resultsC_ourQuest$t_statistic)
hist(resultsC_ourQuest$p_value)
hist(resultsD_ourQuest$t_statistic)
hist(resultsD_ourQuest$p_value)
```

In our question, we tried to check whether the gup of averages between two groups is indeed equivalent to the assignment of the argument "paired = TRUE" in the t-test function of R, hence - to paired sample t-test . We have seen that the results of the simulations in both tests (a t-test on a single sample of mean difference versus a t-test defined for paired samples) are indeed very similar, and there does not seem to be a significant difference, in a manner consistent with the principle taught in class.